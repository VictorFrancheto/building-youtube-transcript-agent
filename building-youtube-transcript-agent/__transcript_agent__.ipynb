{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a391287c",
   "metadata": {},
   "source": [
    "# Developing an Intelligent YouTube Transcript Agent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd782032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain_ollama youtube-transcript-api\n",
    "\n",
    "\n",
    "testando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6579873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLet's get started.\n",
      "\n",
      "Thought: The question is asking for a summary of a video, which means we'll need to use the `generate_summary` tool to create a detailed transcript of the video.\n",
      "\n",
      "Action: generate_summary\n",
      "Action Input: (empty string, since no text has been provided)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mVictor Borges, the speaker, emphasizes that while presenters focus on engaging their audience during a talk, they often overlook the broader impact of reaching a wider online audience through social media and internet sharing. He humorously notes that attention spans have become extinct due to instant-gratification online content. To illustrate this point, he keeps his own TED talk concise, aiming to finish within a minute, and shares a witty joke about why balloons are expensive (inflation).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: This is the start of the process.\n",
      "\n",
      "Action: generate_summary\n",
      "Action Input: (empty string, since no text has been provided)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHere is a single, coherent summary of the text:\n",
      "\n",
      "Victor Borges emphasizes the need for speakers to consider both physical and online audiences when giving talks. He notes that attention spans have decreased significantly since 2009 and suggests keeping TED talks short and concise. To engage his audience, Borges incorporates humor, including a quick joke about balloons.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's my response:\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer: Victor Borges emphasizes the need for speakers to consider both physical and online audiences when giving talks. He notes that attention spans have decreased significantly since 2009 and suggests keeping TED talks short and concise. To engage his audience, Borges incorporates humor, including a quick joke about balloons.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: {'input': 'Can you provide a summary of this video?', 'output': 'Victor Borges emphasizes the need for speakers to consider both physical and online audiences when giving talks. He notes that attention spans have decreased significantly since 2009 and suggests keeping TED talks short and concise. To engage his audience, Borges incorporates humor, including a quick joke about balloons.'}\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for the agent's functionality\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "class YouTubeTranscriptionAgent:\n",
    "    \"\"\"\n",
    "    This class encapsulates the complete workflow to:\n",
    "      1. Extract the video ID and transcript from a YouTube video.\n",
    "      2. Split the transcript into smaller chunks for better processing.\n",
    "      3. Generate a summary, extract main topics, and important quotes using a language model.\n",
    "      4. Process user queries and select the appropriate tool.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"llama3\"):\n",
    "        \"\"\"\n",
    "        Initializes the agent with the name of the language model to be used.\n",
    "        \n",
    "        Args:\n",
    "            model (str): The name of the LLM (default: \"llama3\").\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def extract_video_id(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Extracts the video ID from the YouTube URL.\n",
    "        \n",
    "        Examples of URLs:\n",
    "          - https://www.youtube.com/watch?v=VIDEO_ID\n",
    "          - https://youtu.be/VIDEO_ID\n",
    "        \n",
    "        Args:\n",
    "            url (str): The full video URL.\n",
    "        \n",
    "        Returns:\n",
    "            str: The extracted video ID.\n",
    "        \"\"\"\n",
    "        if \"youtu.be\" in url:\n",
    "            return url.split(\"/\")[-1]\n",
    "        if \"=\" in url:\n",
    "            return url.split(\"=\")[-1]\n",
    "        return url\n",
    "\n",
    "    def get_transcript(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the transcript of the YouTube video and returns a continuous text.\n",
    "        \n",
    "        Steps:\n",
    "          1. Extract the video ID using 'extract_video_id'.\n",
    "          2. Request the transcript via the API.\n",
    "          3. Concatenate the text segments and perform basic cleaning.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The YouTube video URL.\n",
    "        \n",
    "        Returns:\n",
    "            str: The transcript text or an error message if retrieval fails.\n",
    "        \"\"\"\n",
    "        video_id = self.extract_video_id(url)\n",
    "        try:\n",
    "            # Request the transcript (a list of dictionaries with the key \"text\")\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            # Join the text segments into a single string\n",
    "            text = \" \".join([item[\"text\"] for item in transcript])\n",
    "            # Clean up: remove line breaks and single quotes\n",
    "            text = text.replace(\"\\n\", \" \").replace(\"'\", \"\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            return f\"Failed to get the transcript: {str(e)}\"\n",
    "\n",
    "    def create_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> list:\n",
    "        \"\"\"\n",
    "        Splits the text into smaller chunks for easier processing.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The complete text to be split.\n",
    "            chunk_size (int): Approximate number of characters per chunk (default: 1000).\n",
    "            overlap (int): Number of overlapping characters between chunks (default: 100).\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of text chunks.\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    def generate_summary(self, chunks: list) -> str:\n",
    "        \"\"\"\n",
    "        Generates a unified summary from multiple text chunks.\n",
    "        \n",
    "        Steps:\n",
    "          1. For each chunk, generate a mini-summary using the model.\n",
    "          2. Combine all mini-summaries into a single string.\n",
    "          3. Process the combined string to create a final coherent summary in English.\n",
    "        \n",
    "        Args:\n",
    "            chunks (list): A list of text chunks.\n",
    "        \n",
    "        Returns:\n",
    "            str: A coherent summary of the video's content in English.\n",
    "        \"\"\"\n",
    "        llm = OllamaLLM(model=self.model)\n",
    "        \n",
    "        # Template to generate a mini-summary for each chunk\n",
    "        template = \"\"\"Text: {text}\n",
    "Objective: Summarize the provided text.\n",
    "Answer:\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        chain = prompt | llm\n",
    "\n",
    "        # Generate a mini-summary for each chunk\n",
    "        summaries = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "        combined_summary = \" \".join(summaries)\n",
    "\n",
    "        # Template to synthesize the mini-summaries into a final summary\n",
    "        final_template = \"\"\"Multiple summaries: {summaries}\n",
    "Objective: Create a single, coherent summary in English.\n",
    "Answer:\"\"\"\n",
    "        final_prompt = ChatPromptTemplate.from_template(final_template)\n",
    "        final_chain = final_prompt | llm\n",
    "        final_summary = final_chain.invoke({\"summaries\": combined_summary})\n",
    "        return final_summary\n",
    "\n",
    "    def extract_topics(self, chunks: list) -> list:\n",
    "        \"\"\"\n",
    "        Extracts the main topics from the text chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunks (list): A list of text chunks.\n",
    "        \n",
    "        Returns:\n",
    "            list: A unique list of main topics.\n",
    "        \"\"\"\n",
    "        llm = OllamaLLM(model=self.model)\n",
    "        template = \"\"\"Text: {text}\n",
    "Objective: Extract the main topics from the provided text.\n",
    "Answer: List the topics separated by commas.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        chain = prompt | llm\n",
    "\n",
    "        topic_lists = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "        all_topics = set()\n",
    "        for topics in topic_lists:\n",
    "            # Split the response by commas and remove extra spaces\n",
    "            items = [t.strip() for t in topics.split(\",\")]\n",
    "            all_topics.update(items)\n",
    "        # Remove empty items\n",
    "        all_topics = {t for t in all_topics if t}\n",
    "        return list(all_topics)\n",
    "\n",
    "    def extract_quotes(self, chunks: list) -> list:\n",
    "        \"\"\"\n",
    "        Extracts important quotes from the text chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunks (list): A list of text chunks.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list containing unique quotes.\n",
    "        \"\"\"\n",
    "        llm = OllamaLLM(model=self.model)\n",
    "        template = \"\"\"Text: {text}\n",
    "Objective: Extract the most important quote from the text.\n",
    "Answer: Provide the quote as plain text.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        chain = prompt | llm\n",
    "\n",
    "        quotes = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "        unique_quotes = []\n",
    "        seen = set()\n",
    "        for quote in quotes:\n",
    "            normalized = quote.strip().lower()\n",
    "            if normalized and normalized not in seen:\n",
    "                unique_quotes.append(quote.strip())\n",
    "                seen.add(normalized)\n",
    "        return unique_quotes\n",
    "\n",
    "    def process_user_query(self, query: str, chunks: list) -> str:\n",
    "        \"\"\"\n",
    "        Processes the user's query by selecting the appropriate tool (summary, topics, or quotes)\n",
    "        and generates a response.\n",
    "        \n",
    "        Steps:\n",
    "          1. Define wrapper functions for each tool to simplify invocation without parameters.\n",
    "          2. Create a list of Tools with name, function, and description.\n",
    "          3. Initialize a LangChain agent with the defined tools.\n",
    "          4. Invoke the agent with the user's query.\n",
    "          5. If a parsing error occurs, catch the exception and return an error message.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The user's question or instruction.\n",
    "            chunks (list): A list of text chunks from the transcript.\n",
    "        \n",
    "        Returns:\n",
    "            str: The response generated by the agent or an error message in case of failure.\n",
    "        \"\"\"\n",
    "        llm = OllamaLLM(model=self.model)\n",
    "        \n",
    "        # Wrapper functions for each tool\n",
    "        def summary_wrapper(_=\"\"):\n",
    "            return self.generate_summary(chunks)\n",
    "        \n",
    "        def topics_wrapper(_=\"\"):\n",
    "            return self.extract_topics(chunks)\n",
    "        \n",
    "        def quotes_wrapper(_=\"\"):\n",
    "            return self.extract_quotes(chunks)\n",
    "        \n",
    "        tools = [\n",
    "            Tool(\n",
    "                name=\"generate_summary\",\n",
    "                func=summary_wrapper,\n",
    "                description=\"Generates a detailed summary of the transcript.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"extract_topics\",\n",
    "                func=topics_wrapper,\n",
    "                description=\"Extracts the main topics of the transcript.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"extract_quotes\",\n",
    "                func=quotes_wrapper,\n",
    "                description=\"Extracts important quotes from the transcript.\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Initialize the agent with the tools and configure it to use ZERO_SHOT_REACT_DESCRIPTION\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True  # Enables detailed debug messages\n",
    "        )\n",
    "        \n",
    "        # Try to invoke the agent with the query; if a parsing error occurs, catch the exception\n",
    "        try:\n",
    "            response = agent.invoke(input=query, handle_parsing_errors=True)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Agent error: {e}\"\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the agent with the desired model\n",
    "    agent = YouTubeTranscriptionAgent(model=\"llama3\")\n",
    "    \n",
    "    # Example YouTube video URL\n",
    "    example_url = \"https://www.youtube.com/watch?v=1aA1WGON49E\"\n",
    "    \n",
    "    # Retrieve the transcript of the video\n",
    "    transcript_text = agent.get_transcript(example_url)\n",
    "    \n",
    "    # If an error occurs while getting the transcript, display the error message\n",
    "    if transcript_text.startswith(\"Failed to get\"):\n",
    "        print(transcript_text)\n",
    "    else:\n",
    "        # Split the transcript text into chunks for processing\n",
    "        text_chunks = agent.create_chunks(transcript_text)\n",
    "        \n",
    "        # Example user query to get a summary\n",
    "        user_query = \"Can you provide a summary of this video?\"\n",
    "        \n",
    "        # Process the query and get the agent's response\n",
    "        result = agent.process_user_query(user_query, text_chunks)\n",
    "        print(\"Response:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82cfea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1215dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLet's get started.\n",
      "\n",
      "Thought: The input is likely a transcript of a video, and we need to summarize it.\n",
      "\n",
      "Action: get_summary\n",
      "\n",
      "Action Input: [transcript of the video]\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHere's a concise summary:\n",
      "\n",
      "Victor Borges emphasizes the importance of tailoring presentations to appeal to online audiences, acknowledging that attention spans are short and fleeting. He highlights the need for concise and shareable content that can engage people on social media. Using humor, he illustrates his point with an inflation joke and concludes by sharing another joke about balloons being expensive due to \"inflation\" in a clever wrap-up of his own TED talk, which is promised to be under a minute long.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I understand that the goal is to provide a summary of the video transcript.\n",
      "\n",
      "Action: get_summary\n",
      "\n",
      "Action Input: [transcript of the video]\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mVictor Borges emphasizes the significance of considering online reactions to a talk, as attention spans have decreased significantly since 2009. He humorously advises keeping TED talks short and shares a quick joke about balloons being expensive due to inflation.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere are my thoughts and actions:\n",
      "\n",
      "Question: Can you give me a summary of this video?\n",
      "\n",
      "Thought: Let's get started.\n",
      "\n",
      "Thought: The input is likely a transcript of a video, and we need to summarize it.\n",
      "\n",
      "Action: get_summary\n",
      "\n",
      "Action Input: [transcript of the video]\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mVictor Borges emphasizes the need for TED talks to adapt to the internet age, citing the significant decrease in attention spans since 2009. He suggests that speakers should aim to be brief and engaging in order to capture online attention.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI'd be happy to help!\n",
      "\n",
      "Question: Can you give me a summary of this video?\n",
      "\n",
      "Thought: Let's get started.\n",
      "\n",
      "Thought: The input is likely a transcript of a video, and we need to summarize it.\n",
      "\n",
      "Action: get_summary\n",
      "\n",
      "Action Input: [transcript of the video]\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mVictor Borges emphasizes the significance of having an online presence and using social media to reach a broader audience. He highlights the importance of keeping content concise and engaging due to shortening attention spans, making it crucial to adapt to the changing landscape by producing shorter and more captivating presentations that can capture viewers' attention.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI'd be happy to help!\n",
      "\n",
      "Question: Can you give me a summary of this video?\n",
      "\n",
      "Thought: Let's get started.\n",
      "\n",
      "Thought: The input is likely a transcript of a video, and we need to summarize it.\n",
      "\n",
      "Action: get_summary\n",
      "\n",
      "Action Input: [transcript of the video]\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHere is a coherent single summary:\n",
      "\n",
      "Victor Borges emphasizes the importance of speaking to a wider audience through online platforms, rather than solely focusing on an immediate audience. He highlights the need for concise and engaging talks in today's era of declining attention spans, citing TED talks as an example of successful short-form communication.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere are my actions:\n",
      "\n",
      "Question: Can you give me a summary of this video?\n",
      "\n",
      "Thought: Let's get started.\n",
      "\n",
      "Thought: The input is likely a transcript of a video, and we need to summarize it.\n",
      "\n",
      "Action: get_summary\n",
      "\n",
      "Action Input: [transcript of the video]\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mVictor Borges emphasizes the importance of considering the internet audience when giving a talk, noting that attention spans have decreased significantly since 2009. He suggests keeping TED talks concise, with a maximum length of under a minute, and ends his talk by sharing a joke about balloons being expensive due to \"inflation.\"\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's my response:\n",
      "\n",
      "Question: Can you give me a summary of this video?\n",
      "\n",
      "Thought: Let's get started.\n",
      "\n",
      "Thought: The input is likely a transcript of a video, and we need to summarize it.\n",
      "\n",
      "Action: get_summary\n",
      "\n",
      "Action Input: [transcript of the video]\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHere's a single summary:\n",
      "\n",
      "Victor Borges emphasizes the need for effective communication to a broader online audience, highlighting the decline in attention spans since 2009. He advocates for concise and engaging presentations, demonstrating his point by delivering a brief and humorous talk that includes a joke about balloons and inflation.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The final answer is:\n",
      "\n",
      "Here's a summary of this video:\n",
      "\n",
      "Victor Borges emphasizes the need for effective communication to a broader online audience, highlighting the decline in attention spans since 2009. He advocates for concise and engaging presentations, demonstrating his point by delivering a brief and humorous talk that includes a joke about balloons and inflation.\n",
      "\n",
      "I hope it helps!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: {'input': 'Can you give me a summary of this video?', 'output': \"The final answer is:\\n\\nHere's a summary of this video:\\n\\nVictor Borges emphasizes the need for effective communication to a broader online audience, highlighting the decline in attention spans since 2009. He advocates for concise and engaging presentations, demonstrating his point by delivering a brief and humorous talk that includes a joke about balloons and inflation.\\n\\nI hope it helps!\"}\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "def parse_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract video ID from URL.\n",
    "\n",
    "    Args: \n",
    "        url(str): youtube video url\n",
    "\n",
    "    Returns:\n",
    "        Youtube video's video ID\n",
    "    \n",
    "    \"\"\"\n",
    "    if \"youtu.be\" in url:\n",
    "        return url.split(\"/\")[-1]\n",
    "    if \"=\" in url:\n",
    "        return url.split(\"=\")[-1]\n",
    "\n",
    "    return url\n",
    "\n",
    "def get_text_from_video(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Get transcript text from YouTube video.\n",
    "\n",
    "    Args:\n",
    "        url(str): youtube video url\n",
    "\n",
    "    Returns:\n",
    "        Youtube video's transcripted text\n",
    "    \n",
    "    \"\"\"\n",
    "    video_id = parse_url(url)\n",
    "    \n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        transcript_text = \" \".join([entry[\"text\"] for entry in transcript])\n",
    "        transcript_text = transcript_text.replace(\"\\n\", \" \").replace(\"'\", \"\")\n",
    "        return transcript_text\n",
    "    except Exception as e:\n",
    "        return f\"Failed to retrieve transcript: {str(e)}\"\n",
    "\n",
    "def create_chunks(transcript_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Split transcript text into processable chunks.\n",
    "\n",
    "    Args:\n",
    "        transcript_text (str): Youtube video's transcripted text\n",
    "\n",
    "    Returns:\n",
    "        processable chunks\n",
    "    \n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(transcript_text)\n",
    "    return chunks\n",
    "\n",
    "def get_summary(chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Summarize text chunks and create a single summary.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "\n",
    "    Returns:\n",
    "        A single summary for youtube video\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Summarize given text.\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    summaries = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "    \n",
    "    # Better approach to combining summaries\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    \n",
    "    # Create final summary\n",
    "    final_summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Multiple summaries: {summaries}\\nGoal: Create a coherent single summary.\\nAnswer: \"\n",
    "    )\n",
    "    final_summary_chain = final_summary_prompt | llm\n",
    "    final_summary = final_summary_chain.invoke({\"summaries\": combined_summary})\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "def extract_topics(chunks:list) -> list:\n",
    "    \"\"\"\n",
    "    Extract main topics from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "    \n",
    "    Returns:\n",
    "        Main topic list\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Extract main topics from the given text.\n",
    "    Answer: List the key topics separated by commas.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    topics_list = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "\n",
    "    # Combine topics from different chunks\n",
    "    all_topics = set()\n",
    "    for topics in topics_list:\n",
    "        # Split comma-separated topics and clean whitespace\n",
    "        topic_items = [t.strip() for t in topics.split(\",\")]\n",
    "        all_topics.update(topic_items)\n",
    "\n",
    "    # Remove empty elements\n",
    "    all_topics = {topic for topic in all_topics if topic}\n",
    "    \n",
    "    return list(all_topics)\n",
    "\n",
    "def extract_quotes(chunks:list) -> list:\n",
    "    \"\"\"\n",
    "    Extract important quotes from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "    \n",
    "    Returns:\n",
    "        important quotes list\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Extract the most important quote from this text.\n",
    "    Answer: Provide the quote as plain text.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    quotes = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "    \n",
    "    # Filter duplicate or empty quotes\n",
    "    unique_quotes = []\n",
    "    seen_quotes = set()\n",
    "    \n",
    "    for quote in quotes:\n",
    "        # Normalize quote (clean whitespace and compare lowercase)\n",
    "        normalized = quote.strip().lower()\n",
    "        if normalized and normalized not in seen_quotes:\n",
    "            unique_quotes.append(quote.strip())\n",
    "            seen_quotes.add(normalized)\n",
    "    \n",
    "    return unique_quotes\n",
    "\n",
    "def process_user_query(query, chunks, url):\n",
    "    \"\"\"Select appropriate tool based on user query and generate response.\"\"\"\n",
    "\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    \n",
    "    # Create wrapper functions for tools\n",
    "    def get_summary_wrapper(input_str=\"\"):\n",
    "        return get_summary(chunks)\n",
    "    \n",
    "    def extract_topics_wrapper(input_str=\"\"):\n",
    "        return extract_topics(chunks)\n",
    "    \n",
    "    def extract_quotes_wrapper(input_str=\"\"):\n",
    "        return extract_quotes(chunks)\n",
    "    \n",
    "    # Create tools with wrapper functions\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"get_summary\",\n",
    "            func=get_summary_wrapper,\n",
    "            description=\"Provides a detailed summary of the transcript.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"extract_topics\",\n",
    "            func=extract_topics_wrapper,\n",
    "            description=\"Extracts main topics from the transcript.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"extract_quotes\",\n",
    "            func=extract_quotes_wrapper,\n",
    "            description=\"Extracts important quotes from the transcript.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Call agent with user query\n",
    "    response = agent.invoke(input=query, handle_parsing_errors=True)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize LLM model\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "    # Example YouTube URL\n",
    "    url = \"https://www.youtube.com/watch?v=1aA1WGON49E\"\n",
    "\n",
    "    transcript_text = get_text_from_video(url)\n",
    "\n",
    "    chunks = create_chunks(transcript_text)\n",
    "\n",
    "    user_query = \"Can you give me a summary of this video?\"\n",
    "    \n",
    "    result = process_user_query(user_query, chunks, url)\n",
    "    print(f\"Response: {result}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
