{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd782032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain_ollama youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8fbe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract video ID from URL.\n",
    "\n",
    "    Args: \n",
    "        url(str): youtube video url\n",
    "\n",
    "    Returns:\n",
    "        Youtube video's video ID\n",
    "    \n",
    "    \"\"\"\n",
    "    if \"=\" in url:\n",
    "        return url.split(\"=\")[-1]\n",
    "\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_text_from_video(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Get transcript text from YouTube video.\n",
    "\n",
    "    Args:\n",
    "        url(str): youtube video url\n",
    "\n",
    "    Returns:\n",
    "        Youtube video's transcripted text\n",
    "    \n",
    "    \"\"\"\n",
    "    video_id = parse_url(url)\n",
    "    \n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        transcript_text = \" \".join([entry[\"text\"] for entry in transcript])\n",
    "        transcript_text = transcript_text.replace(\"\\n\", \" \").replace(\"'\", \"\")\n",
    "        return transcript_text\n",
    "    except Exception as e:\n",
    "        return f\"Failed to retrieve transcript: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f2388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(transcript_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Split transcript text into processable chunks.\n",
    "\n",
    "    Args:\n",
    "        transcript_text (str): Youtube video's transcripted text\n",
    "\n",
    "    Returns:\n",
    "        processable chunks\n",
    "    \n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(transcript_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c3ca23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Summarize text chunks and create a single summary.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "\n",
    "    Returns:\n",
    "        A single summary for youtube video\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Summarize given text.\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    summaries = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "    \n",
    "    combined_summary = \" \".join(summaries)\n",
    "    \n",
    "    # Create final summary\n",
    "    final_summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Multiple summaries: {summaries}\\nGoal: Create a coherent single summary.\\nAnswer: \"\n",
    "    )\n",
    "    final_summary_chain = final_summary_prompt | llm\n",
    "    final_summary = final_summary_chain.invoke({\"summaries\": combined_summary})\n",
    "    \n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(chunks:list) -> list:\n",
    "    \"\"\"\n",
    "    Extract main topics from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "    \n",
    "    Returns:\n",
    "        Main topic list\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Extract main topics from the given text.\n",
    "    Answer: List the key topics separated by commas.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    topics_list = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "\n",
    "    # Combine topics from different chunks\n",
    "    all_topics = set()\n",
    "    for topics in topics_list:\n",
    "        # Split comma-separated topics and clean whitespace\n",
    "        topic_items = [t.strip() for t in topics.split(\",\")]\n",
    "        all_topics.update(topic_items)\n",
    "\n",
    "    # Remove empty elements\n",
    "    all_topics = {topic for topic in all_topics if topic}\n",
    "    \n",
    "    return list(all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9251d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quotes(chunks:list) -> list:\n",
    "    \"\"\"\n",
    "    Extract important quotes from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "    \n",
    "    Returns:\n",
    "        important quotes list\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Extract the most important quote from this text.\n",
    "    Answer: Provide the quote as plain text.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    quotes = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "    \n",
    "    # Filter duplicate or empty quotes\n",
    "    unique_quotes = []\n",
    "    seen_quotes = set()\n",
    "    \n",
    "    for quote in quotes:\n",
    "        # Normalize quote (clean whitespace and compare lowercase)\n",
    "        normalized = quote.strip().lower()\n",
    "        if normalized and normalized not in seen_quotes:\n",
    "            unique_quotes.append(quote.strip())\n",
    "            seen_quotes.add(normalized)\n",
    "    \n",
    "    return unique_quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_query(query, chunks, url):\n",
    "    \"\"\"Select appropriate tool based on user query and generate response.\"\"\"\n",
    "\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    \n",
    "    # Create wrapper functions for tools\n",
    "    def get_summary_wrapper(input_str=\"\"):\n",
    "        return get_summary(chunks)\n",
    "    \n",
    "    def extract_topics_wrapper(input_str=\"\"):\n",
    "        return extract_topics(chunks)\n",
    "    \n",
    "    def extract_quotes_wrapper(input_str=\"\"):\n",
    "        return extract_quotes(chunks)\n",
    "    \n",
    "    # Create tools with wrapper functions\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"get_summary\",\n",
    "            func=get_summary_wrapper,\n",
    "            description=\"Provides a detailed summary of the transcript.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"extract_topics\",\n",
    "            func=extract_topics_wrapper,\n",
    "            description=\"Extracts main topics from the transcript.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"extract_quotes\",\n",
    "            func=extract_quotes_wrapper,\n",
    "            description=\"Extracts important quotes from the transcript.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Call agent with user query\n",
    "    response = agent.invoke(input=query, handle_parsing_errors=True)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create wrapper functions for tools\n",
    "    def get_summary_wrapper(input_str=\"\"):\n",
    "        return get_summary(chunks)\n",
    "    \n",
    "    def extract_topics_wrapper(input_str=\"\"):\n",
    "        return extract_topics(chunks)\n",
    "    \n",
    "    def extract_quotes_wrapper(input_str=\"\"):\n",
    "        return extract_quotes(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG USAGE\n",
    "Tool(\n",
    "      name=\"extract_quotes\",\n",
    "      func=get_summary(chunks),\n",
    "      description=\"Extracts important quotes from the transcript.\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(input=query, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd86ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2390c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "    url = \"youtube_url\"\n",
    "\n",
    "    transcript_text = get_text_from_video(url)\n",
    "\n",
    "    chunks = create_chunks(transcript_text)\n",
    "\n",
    "    user_query = \"Can you give me a summary of this video?\"\n",
    "    \n",
    "    result = process_user_query(user_query, chunks, url)\n",
    "    print(f\"Response: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
