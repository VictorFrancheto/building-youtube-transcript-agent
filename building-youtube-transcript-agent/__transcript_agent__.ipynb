{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd782032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain_ollama youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1215dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_18020\\1749225893.py:194: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLet's get started!\n",
      "\n",
      "Thought: The question is asking for a summary of a video, which means we need to analyze the transcript and extract important information.\n",
      "\n",
      "Action: get_summary\n",
      "Action Input: (assuming there is no input transcript provided)\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHere's a coherent single summary:\n",
      "\n",
      "Victor Borges emphasizes the need for presenters to consider how their message will be received by a wider online audience, not just the immediate crowd in front of them. He notes that attention spans have decreased significantly, making concise and brief content more engaging.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLet's continue!\n",
      "\n",
      "Action: extract_topics\n",
      "Action Input: (assuming there is no input transcript provided)\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m['Here are the main topics extracted from the text:\\n\\nAudience', 'Time Constraints', 'Social Media', 'TED Talks', \"Humor.\\n\\nLet me know if you'd like me to elaborate on any of these points!\", 'Attention Span']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLet's continue!\n",
      "\n",
      "Action: extract_quotes\n",
      "Action Input: (assuming there is no input transcript provided)\n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m['\"I care what the internet thinks of my talk. Because they are the ones who get it seen and shared.\"']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mLet's wrap up the process.\n",
      "\n",
      "Thought: We've done all we can with this video, let's summarize our findings!\n",
      "\n",
      "Action: None\n",
      "Action Input: (no input needed)\u001b[0m\n",
      "Observation: None is not a valid tool, try one of [get_summary, extract_topics, extract_quotes].\n",
      "Thought:\u001b[32;1m\u001b[1;3mLet's wrap up the process.\n",
      "\n",
      "Thought: We've done all we can with this video, let's summarize our findings!\n",
      "\n",
      "Final Answer: The summary of the video is that Victor Borges emphasizes the importance of considering the online audience when presenting, as attention spans have decreased. He also notes that concise and brief content is more engaging.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: {'input': 'Can you give me a summary of this video?', 'output': 'The summary of the video is that Victor Borges emphasizes the importance of considering the online audience when presenting, as attention spans have decreased. He also notes that concise and brief content is more engaging.'}\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "\n",
    "def parse_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract video ID from URL.\n",
    "\n",
    "    Args: \n",
    "        url(str): youtube video url\n",
    "\n",
    "    Returns:\n",
    "        Youtube video's video ID\n",
    "    \n",
    "    \"\"\"\n",
    "    if \"youtu.be\" in url:\n",
    "        return url.split(\"/\")[-1]\n",
    "    if \"=\" in url:\n",
    "        return url.split(\"=\")[-1]\n",
    "\n",
    "    return url\n",
    "\n",
    "def get_text_from_video(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Get transcript text from YouTube video.\n",
    "\n",
    "    Args:\n",
    "        url(str): youtube video url\n",
    "\n",
    "    Returns:\n",
    "        Youtube video's transcripted text\n",
    "    \n",
    "    \"\"\"\n",
    "    video_id = parse_url(url)\n",
    "    \n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        transcript_text = \" \".join([entry[\"text\"] for entry in transcript])\n",
    "        transcript_text = transcript_text.replace(\"\\n\", \" \").replace(\"'\", \"\")\n",
    "        return transcript_text\n",
    "    except Exception as e:\n",
    "        return f\"Failed to retrieve transcript: {str(e)}\"\n",
    "\n",
    "def create_chunks(transcript_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Split transcript text into processable chunks.\n",
    "\n",
    "    Args:\n",
    "        transcript_text (str): Youtube video's transcripted text\n",
    "\n",
    "    Returns:\n",
    "        processable chunks\n",
    "    \n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_text(transcript_text)\n",
    "    return chunks\n",
    "\n",
    "def get_summary(chunks: list) -> str:\n",
    "    \"\"\"\n",
    "    Summarize text chunks and create a single summary.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "\n",
    "    Returns:\n",
    "        A single summary for youtube video\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Summarize given text.\n",
    "    Answer: \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    summaries = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "    \n",
    "    # Better approach to combining summaries\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    \n",
    "    # Create final summary\n",
    "    final_summary_prompt = ChatPromptTemplate.from_template(\n",
    "        \"Multiple summaries: {summaries}\\nGoal: Create a coherent single summary.\\nAnswer: \"\n",
    "    )\n",
    "    final_summary_chain = final_summary_prompt | llm\n",
    "    final_summary = final_summary_chain.invoke({\"summaries\": combined_summary})\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "def extract_topics(chunks:list) -> list:\n",
    "    \"\"\"\n",
    "    Extract main topics from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "    \n",
    "    Returns:\n",
    "        Main topic list\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Extract main topics from the given text.\n",
    "    Answer: List the key topics separated by commas.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    topics_list = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "\n",
    "    # Combine topics from different chunks\n",
    "    all_topics = set()\n",
    "    for topics in topics_list:\n",
    "        # Split comma-separated topics and clean whitespace\n",
    "        topic_items = [t.strip() for t in topics.split(\",\")]\n",
    "        all_topics.update(topic_items)\n",
    "\n",
    "    # Remove empty elements\n",
    "    all_topics = {topic for topic in all_topics if topic}\n",
    "    \n",
    "    return list(all_topics)\n",
    "\n",
    "def extract_quotes(chunks:list) -> list:\n",
    "    \"\"\"\n",
    "    Extract important quotes from text chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): processable chunks of transcriptted text\n",
    "    \n",
    "    Returns:\n",
    "        important quotes list\n",
    "    \"\"\"\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    template = \"\"\"Text: {text}\n",
    "    Goal: Extract the most important quote from this text.\n",
    "    Answer: Provide the quote as plain text.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    quotes = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "    \n",
    "    # Filter duplicate or empty quotes\n",
    "    unique_quotes = []\n",
    "    seen_quotes = set()\n",
    "    \n",
    "    for quote in quotes:\n",
    "        # Normalize quote (clean whitespace and compare lowercase)\n",
    "        normalized = quote.strip().lower()\n",
    "        if normalized and normalized not in seen_quotes:\n",
    "            unique_quotes.append(quote.strip())\n",
    "            seen_quotes.add(normalized)\n",
    "    \n",
    "    return unique_quotes\n",
    "\n",
    "def process_user_query(query, chunks, url):\n",
    "    \"\"\"Select appropriate tool based on user query and generate response.\"\"\"\n",
    "\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "    \n",
    "    # Create wrapper functions for tools\n",
    "    def get_summary_wrapper(input_str=\"\"):\n",
    "        return get_summary(chunks)\n",
    "    \n",
    "    def extract_topics_wrapper(input_str=\"\"):\n",
    "        return extract_topics(chunks)\n",
    "    \n",
    "    def extract_quotes_wrapper(input_str=\"\"):\n",
    "        return extract_quotes(chunks)\n",
    "    \n",
    "    # Create tools with wrapper functions\n",
    "    tools = [\n",
    "        Tool(\n",
    "            name=\"get_summary\",\n",
    "            func=get_summary_wrapper,\n",
    "            description=\"Provides a detailed summary of the transcript.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"extract_topics\",\n",
    "            func=extract_topics_wrapper,\n",
    "            description=\"Extracts main topics from the transcript.\"\n",
    "        ),\n",
    "        Tool(\n",
    "            name=\"extract_quotes\",\n",
    "            func=extract_quotes_wrapper,\n",
    "            description=\"Extracts important quotes from the transcript.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # Call agent with user query\n",
    "    response = agent.invoke(input=query, handle_parsing_errors=True)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize LLM model\n",
    "    llm = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "    # Example YouTube URL\n",
    "    url = \"https://www.youtube.com/watch?v=1aA1WGON49E\"\n",
    "\n",
    "    transcript_text = get_text_from_video(url)\n",
    "\n",
    "    chunks = create_chunks(transcript_text)\n",
    "\n",
    "    user_query = \"Can you give me a summary of this video?\"\n",
    "    \n",
    "    result = process_user_query(user_query, chunks, url)\n",
    "    print(f\"Response: {result}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
