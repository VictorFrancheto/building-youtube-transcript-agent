{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a391287c",
   "metadata": {},
   "source": [
    "# Developing an Intelligent YouTube Transcript Agent\n",
    "\n",
    "The goal of this project is to develop an intelligent agent capable of transcribing and summarizing YouTube videos, extracting key information clearly and concisely. The objective is to enable users to quickly obtain the most relevant insights from the content without having to watch the entire video.\n",
    "\n",
    "This agent leverages advanced language models to process subtitles, identify the most important segments, and generate a summary in Portuguese, even if the original video is in English. With this tool, consuming knowledge becomes more efficient, saving time and ensuring that no essential points are missed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f2259",
   "metadata": {},
   "source": [
    "### Importing libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain langchain_ollama youtube-transcript-api\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6bac5c",
   "metadata": {},
   "source": [
    "### Installing Ollama and Downloading the Model\n",
    "\n",
    "Since we will be using the Ollama model, you need to have Ollama installed on your computer. If you haven't installed it yet, follow the steps below:\n",
    "\n",
    "- **Download and install Ollama**: [Ollama Official Website](https://ollama.com/)\n",
    "- **Download the model to your system**: Run the following command in your terminal: `ollama pull llama3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c2d1b3",
   "metadata": {},
   "source": [
    "Before presenting the agent's implementation, let's first explain how the defined class works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb4c06",
   "metadata": {},
   "source": [
    "### YouTube Transcription Agent (`Class`)\n",
    "\n",
    "This class implements an intelligent agent capable of transcribing and summarizing YouTube videos. It follows a structured workflow to extract key information efficiently.\n",
    "\n",
    "#### Features\n",
    "\n",
    "- **Extract Video ID**: Retrieves the unique identifier from a YouTube URL.\n",
    "- **Get Transcript**: Uses an API to fetch and clean the video's transcript.\n",
    "- **Chunk Splitting**: Breaks the transcript into smaller parts for better processing.\n",
    "- **Generate Summary**: Creates a concise summary from multiple transcript chunks.\n",
    "- **Extract Topics**: Identifies the main subjects discussed in the video.\n",
    "- **Extract Quotes**: Finds significant quotes from the transcript.\n",
    "- **User Query Processing**: Allows users to request summaries, topics, or quotes dynamically.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. **Provide a YouTube URL** – The agent extracts the transcript.\n",
    "2. **Process the Text** – The transcript is divided into chunks.\n",
    "3. **Generate Insights** – The agent summarizes, extracts key topics, and highlights quotes.\n",
    "4. **Answer User Queries** – Users can ask for specific information using a language model.\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "```python\n",
    "# Initialize the agent with a language model\n",
    "agent = YouTubeTranscriptionAgent(model=\"llama3\")\n",
    "\n",
    "# Provide a YouTube video URL\n",
    "video_url = \"https://www.youtube.com/watch?v=1aA1WGON49E\"\n",
    "\n",
    "# Retrieve the transcript and process it\n",
    "transcript = agent.get_transcript(video_url)\n",
    "if not transcript.startswith(\"Failed to get\"):\n",
    "    chunks = agent.create_chunks(transcript)\n",
    "    \n",
    "    # Ask for a summary\n",
    "    query = \"Can you summarize this video?\"\n",
    "    response = agent.process_user_query(query, chunks)\n",
    "    \n",
    "    print(\"Summary:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLet's get started.\n",
      "\n",
      "Thought: The question is asking for a summary of a video, which means we'll need to use the `generate_summary` tool to create a detailed transcript of the video.\n",
      "\n",
      "Action: generate_summary\n",
      "Action Input: (empty string, since no text has been provided)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mVictor Borges, the speaker, emphasizes that while presenters focus on engaging their audience during a talk, they often overlook the broader impact of reaching a wider online audience through social media and internet sharing. He humorously notes that attention spans have become extinct due to instant-gratification online content. To illustrate this point, he keeps his own TED talk concise, aiming to finish within a minute, and shares a witty joke about why balloons are expensive (inflation).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: This is the start of the process.\n",
      "\n",
      "Action: generate_summary\n",
      "Action Input: (empty string, since no text has been provided)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHere is a single, coherent summary of the text:\n",
      "\n",
      "Victor Borges emphasizes the need for speakers to consider both physical and online audiences when giving talks. He notes that attention spans have decreased significantly since 2009 and suggests keeping TED talks short and concise. To engage his audience, Borges incorporates humor, including a quick joke about balloons.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mHere's my response:\n",
      "\n",
      "Thought: I now know the final answer.\n",
      "\n",
      "Final Answer: Victor Borges emphasizes the need for speakers to consider both physical and online audiences when giving talks. He notes that attention spans have decreased significantly since 2009 and suggests keeping TED talks short and concise. To engage his audience, Borges incorporates humor, including a quick joke about balloons.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Response: {'input': 'Can you provide a summary of this video?', 'output': 'Victor Borges emphasizes the need for speakers to consider both physical and online audiences when giving talks. He notes that attention spans have decreased significantly since 2009 and suggests keeping TED talks short and concise. To engage his audience, Borges incorporates humor, including a quick joke about balloons.'}\n"
     ]
    }
   ],
   "source": [
    "class YouTubeTranscriptionAgent:\n",
    "    \"\"\"\n",
    "    This class encapsulates the complete workflow to:\n",
    "      1. Extract the video ID and transcript from a YouTube video.\n",
    "      2. Split the transcript into smaller chunks for better processing.\n",
    "      3. Generate a summary, extract main topics, and important quotes using a language model.\n",
    "      4. Process user queries and select the appropriate tool.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"llama3\"):\n",
    "        \"\"\"\n",
    "        Initializes the agent with the name of the language model to be used.\n",
    "        \n",
    "        Args:\n",
    "            model (str): The name of the LLM (default: \"llama3\").\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "\n",
    "    def extract_video_id(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Extracts the video ID from the YouTube URL.\n",
    "        \n",
    "        Examples of URLs:\n",
    "          - https://www.youtube.com/watch?v=VIDEO_ID\n",
    "          - https://youtu.be/VIDEO_ID\n",
    "        \n",
    "        Args:\n",
    "            url (str): The full video URL.\n",
    "        \n",
    "        Returns:\n",
    "            str: The extracted video ID.\n",
    "        \"\"\"\n",
    "        if \"youtu.be\" in url:\n",
    "            return url.split(\"/\")[-1]\n",
    "        if \"=\" in url:\n",
    "            return url.split(\"=\")[-1]\n",
    "        return url\n",
    "\n",
    "    def get_transcript(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieves the transcript of the YouTube video and returns a continuous text.\n",
    "        \n",
    "        Steps:\n",
    "          1. Extract the video ID using 'extract_video_id'.\n",
    "          2. Request the transcript via the API.\n",
    "          3. Concatenate the text segments and perform basic cleaning.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The YouTube video URL.\n",
    "        \n",
    "        Returns:\n",
    "            str: The transcript text or an error message if retrieval fails.\n",
    "        \"\"\"\n",
    "        video_id = self.extract_video_id(url)\n",
    "        try:\n",
    "            # Request the transcript (a list of dictionaries with the key \"text\")\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            # Join the text segments into a single string\n",
    "            text = \" \".join([item[\"text\"] for item in transcript])\n",
    "            # Clean up: remove line breaks and single quotes\n",
    "            text = text.replace(\"\\n\", \" \").replace(\"'\", \"\")\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            return f\"Failed to get the transcript: {str(e)}\"\n",
    "\n",
    "    def create_chunks(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> list:\n",
    "        \"\"\"\n",
    "        Splits the text into smaller chunks for easier processing.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The complete text to be split.\n",
    "            chunk_size (int): Approximate number of characters per chunk (default: 1000).\n",
    "            overlap (int): Number of overlapping characters between chunks (default: 100).\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of text chunks.\n",
    "        \"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "    def generate_summary(self, chunks: list) -> str:\n",
    "        \"\"\"\n",
    "        Generates a unified summary from multiple text chunks.\n",
    "        \n",
    "        Steps:\n",
    "          1. For each chunk, generate a mini-summary using the model.\n",
    "          2. Combine all mini-summaries into a single string.\n",
    "          3. Process the combined string to create a final coherent summary in English.\n",
    "        \n",
    "        Args:\n",
    "            chunks (list): A list of text chunks.\n",
    "        \n",
    "        Returns:\n",
    "            str: A coherent summary of the video's content in English.\n",
    "        \"\"\"\n",
    "        llm = OllamaLLM(model=self.model)\n",
    "        \n",
    "        # Template to generate a mini-summary for each chunk\n",
    "        template = \"\"\"Text: {text}\n",
    "Objective: Summarize the provided text.\n",
    "Answer:\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        chain = prompt | llm\n",
    "\n",
    "        # Generate a mini-summary for each chunk\n",
    "        summaries = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "        combined_summary = \" \".join(summaries)\n",
    "\n",
    "        # Template to synthesize the mini-summaries into a final summary\n",
    "        final_template = \"\"\"Multiple summaries: {summaries}\n",
    "Objective: Create a single, coherent summary in English.\n",
    "Answer:\"\"\"\n",
    "        final_prompt = ChatPromptTemplate.from_template(final_template)\n",
    "        final_chain = final_prompt | llm\n",
    "        final_summary = final_chain.invoke({\"summaries\": combined_summary})\n",
    "        return final_summary\n",
    "\n",
    "    def extract_topics(self, chunks: list) -> list:\n",
    "        \"\"\"\n",
    "        Extracts the main topics from the text chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunks (list): A list of text chunks.\n",
    "        \n",
    "        Returns:\n",
    "            list: A unique list of main topics.\n",
    "        \"\"\"\n",
    "        llm = OllamaLLM(model=self.model)\n",
    "        template = \"\"\"Text: {text}\n",
    "Objective: Extract the main topics from the provided text.\n",
    "Answer: List the topics separated by commas.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        chain = prompt | llm\n",
    "\n",
    "        topic_lists = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "        all_topics = set()\n",
    "        for topics in topic_lists:\n",
    "            # Split the response by commas and remove extra spaces\n",
    "            items = [t.strip() for t in topics.split(\",\")]\n",
    "            all_topics.update(items)\n",
    "        # Remove empty items\n",
    "        all_topics = {t for t in all_topics if t}\n",
    "        return list(all_topics)\n",
    "\n",
    "    def extract_quotes(self, chunks: list) -> list:\n",
    "        \"\"\"\n",
    "        Extracts important quotes from the text chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunks (list): A list of text chunks.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list containing unique quotes.\n",
    "        \"\"\"\n",
    "        llm = OllamaLLM(model=self.model)\n",
    "        template = \"\"\"Text: {text}\n",
    "Objective: Extract the most important quote from the text.\n",
    "Answer: Provide the quote as plain text.\"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "        chain = prompt | llm\n",
    "\n",
    "        quotes = [chain.invoke({\"text\": chunk}) for chunk in chunks]\n",
    "        unique_quotes = []\n",
    "        seen = set()\n",
    "        for quote in quotes:\n",
    "            normalized = quote.strip().lower()\n",
    "            if normalized and normalized not in seen:\n",
    "                unique_quotes.append(quote.strip())\n",
    "                seen.add(normalized)\n",
    "        return unique_quotes\n",
    "\n",
    "    def process_user_query(self, query: str, chunks: list) -> str:\n",
    "        \"\"\"\n",
    "        Processes the user's query by selecting the appropriate tool (summary, topics, or quotes)\n",
    "        and generates a response.\n",
    "        \n",
    "        Steps:\n",
    "          1. Define wrapper functions for each tool to simplify invocation without parameters.\n",
    "          2. Create a list of Tools with name, function, and description.\n",
    "          3. Initialize a LangChain agent with the defined tools.\n",
    "          4. Invoke the agent with the user's query.\n",
    "          5. If a parsing error occurs, catch the exception and return an error message.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The user's question or instruction.\n",
    "            chunks (list): A list of text chunks from the transcript.\n",
    "        \n",
    "        Returns:\n",
    "            str: The response generated by the agent or an error message in case of failure.\n",
    "        \"\"\"\n",
    "        llm = OllamaLLM(model=self.model)\n",
    "        \n",
    "        # Wrapper functions for each tool\n",
    "        def summary_wrapper(_=\"\"):\n",
    "            return self.generate_summary(chunks)\n",
    "        \n",
    "        def topics_wrapper(_=\"\"):\n",
    "            return self.extract_topics(chunks)\n",
    "        \n",
    "        def quotes_wrapper(_=\"\"):\n",
    "            return self.extract_quotes(chunks)\n",
    "        \n",
    "        tools = [\n",
    "            Tool(\n",
    "                name=\"generate_summary\",\n",
    "                func=summary_wrapper,\n",
    "                description=\"Generates a detailed summary of the transcript.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"extract_topics\",\n",
    "                func=topics_wrapper,\n",
    "                description=\"Extracts the main topics of the transcript.\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"extract_quotes\",\n",
    "                func=quotes_wrapper,\n",
    "                description=\"Extracts important quotes from the transcript.\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Initialize the agent with the tools and configure it to use ZERO_SHOT_REACT_DESCRIPTION\n",
    "        agent = initialize_agent(\n",
    "            tools=tools,\n",
    "            llm=llm,\n",
    "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            verbose=True  # Enables detailed debug messages\n",
    "        )\n",
    "        \n",
    "        # Try to invoke the agent with the query; if a parsing error occurs, catch the exception\n",
    "        try:\n",
    "            response = agent.invoke(input=query, handle_parsing_errors=True)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Agent error: {e}\"\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the agent with the desired model\n",
    "    agent = YouTubeTranscriptionAgent(model=\"llama3\")\n",
    "    \n",
    "    # Example YouTube video URL\n",
    "    example_url = \"https://www.youtube.com/watch?v=1aA1WGON49E\"\n",
    "    \n",
    "    # Retrieve the transcript of the video\n",
    "    transcript_text = agent.get_transcript(example_url)\n",
    "    \n",
    "    # If an error occurs while getting the transcript, display the error message\n",
    "    if transcript_text.startswith(\"Failed to get\"):\n",
    "        print(transcript_text)\n",
    "    else:\n",
    "        # Split the transcript text into chunks for processing\n",
    "        text_chunks = agent.create_chunks(transcript_text)\n",
    "        \n",
    "        # Example user query to get a summary\n",
    "        user_query = \"Can you provide a summary of this video?\"\n",
    "        \n",
    "        # Process the query and get the agent's response\n",
    "        result = agent.process_user_query(user_query, text_chunks)\n",
    "        print(\"Response:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
